{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_API_KEY = \"sk-jM9iGBxOO_aKjZCoWlVG9A\"\n",
    "PROXY_ENDPOINT = \"https://nova-litellm-proxy.onrender.com\"\n",
    "CARTESIA_API_KEY = \"3b674e04-346d-444b-8c99-43e2a18d7aa2\"\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "gpt = OpenAI(\n",
    "        api_key=TEAM_API_KEY, # set this!!!\n",
    "        base_url=PROXY_ENDPOINT # and this!!!\n",
    "    )\n",
    "\n",
    "TEMPERATURE = 0.5\n",
    "MAX_TOKENS = 100\n",
    "TOP_P = 1.0\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from cartesia import Cartesia\n",
    "CARTESIA_VOICE_ID = \"794f9389-aac1-45b6-b726-9d9369183238\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Speech to Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wav2txt(file_path):\n",
    "    \"\"\"\n",
    "    Converts a .wav audio file to str using OpenAI's Whisper model.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .wav file to transcribe.\n",
    "\n",
    "    Returns:\n",
    "        str: Transcribed text from the audio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the audio file\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        # Use Whisper API to transcribe the audio\n",
    "        response = gpt.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    \n",
    "    # Return the transcribed text\n",
    "    return response.text\n",
    "\n",
    "def txt2wav(text, file_path):\n",
    "    \"\"\"\"\n",
    "    Converts text to speech using Cartesia's Sonic model and saves the audio as a .wav file.\n",
    "    \"\"\"\n",
    "    \n",
    "    voice = Cartesia(api_key=CARTESIA_API_KEY)\n",
    "\n",
    "    data = voice.tts.bytes(\n",
    "        model_id=\"sonic-english\",\n",
    "        transcript=text,\n",
    "        voice_id= CARTESIA_VOICE_ID,\n",
    "        output_format={\n",
    "            \"container\": \"wav\",\n",
    "            \"encoding\": \"pcm_f32le\",\n",
    "            \"sample_rate\": 44100,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicken nuggets.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2wav(\"chicken nuggets\", \"response.wav\")\n",
    "\n",
    "wav2txt(\"/Users/kyuminpark/Documents/NOVA/response.wav\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get GPT response\n",
    "def get_gpt_response(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Initial question to user\n",
    "initial_question_to_user = \"\"\"\n",
    "Hey Kelly, it's so nice to hear from you! \n",
    "Would you mind sharing how your day has been so far? \n",
    "I'd love to hear what you've been up to today.\"\n",
    "\"\"\"\n",
    "\n",
    "response_to_inital_question = \"\"\" \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "initial_context = \"\"\"\n",
    "You are a dementia care specialist who is having a casual conversation with a patient to help them recall their day. \n",
    "Your ultimate objective as the specialist is to retell a story that most accurately describes the events that happened to them that day. \n",
    "This story will be used in the future to help dementia patients remember what they did and who they are. \n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "# Questions asking for elaboration on details of their day\n",
    "\n",
    "prompt_1_context = f\"\"\"\n",
    "You are a dementia care specialist who is having a casual conversation with a patient to help them recall their day. \n",
    "Your ultimate objective as the specialist is to retell a story that most accurately describes the events that happened to them that day. \n",
    "This story will be used in the future to help dementia patients remember what they did and who they are. \n",
    "So, this is the initial conversation starter you led with: {initial_question_to_user}. \n",
    "This is their response to your initial conversation starter: {response_to_inital_question}”\n",
    "\"\"\"\n",
    "\n",
    "prompt_1_example_1 = \"\"\"\n",
    "If the patient talks about a walk in the park, you can respond with 'That sounds so peaceful! \n",
    "What's the park like around this time of year? Are there lots of flowers or maybe some colorful leaves?' \n",
    "\"\"\"\n",
    "\n",
    "prompt_1_example_2 = \"\"\"\n",
    "If the patient talks about running into an old friend, you can respond with, 'How wonderful to run into an old friend! \n",
    "Did you both get a chance to catch up? I'd love to hear more about it.'\n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "Your next task as the dementia care specialist is to ask follow-up questions based on their response for more details about the events they talked about. \n",
    "Make sure you first respond with a nice comment. For example, {prompt_1_example_1}. Another example is, {prompt_1_example_2}.\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "# Questions targeted towards asking how they’re feeling\n",
    "\n",
    "prompt_2_example_1 = \"Looking back on your day, was there anything that made you feel happy or content?\"\n",
    "prompt_2_example_2 = \"Was there something you felt thankful for today, no matter how small?\"\n",
    "prompt_2_example_3 = \"What did you find yourself thinking about today?\"\n",
    "\n",
    "prompt_2 = \"\"\"\n",
    "Now that you as a dementia care specialist have a good understanding of how the dementia patient's day went, \n",
    "we now want to ask follow-up questions that help explore their feelings, reflections, or experiences more deeply. \n",
    "For example, {prompt_2_example_1}. Another example is, {prompt_2_example_2}. Another example is, {prompt_2_example_3}.\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "# Questions asking they’re looking forward to\n",
    "\n",
    "prompt_3_example_1 = \"Are there any activities or events you're excited about in the next few days?\"\n",
    "prompt_3_example_2 = \"Do you have any plans that you're thinking about? Even something small, like a favorite meal or a nice walk?\"\n",
    "prompt_3_example_3 = \"Is there something you'd like to do soon that would make you happy, like going to a park or enjoying a favorite hobby?\"\n",
    "\n",
    "prompt_3 = \"\"\"\n",
    "After gaining a good understanding of their general feelings, reflections, and experiences, let's move the conversation toward the future. \n",
    "Ask questions that can help deepen the conversation and explore their hopes, interests, and plans, even if they're small. \n",
    "For example, {prompt_3_example_1}. Another example is, {prompt_3_example_3}. Another example is, {prompt_3_example_3}.\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "# Questions asking for a reflection on who they are as a person\n",
    "\n",
    "prompt_4_example_1 = \"After talking about your day and looking ahead, what's one thing about yourself that you're proud of?\"\n",
    "prompt_4_example_2 = \"As we've talked about your past and your hopes for the future, what do you think people will remember most about you?\"\n",
    "prompt_4_example_3 = \"Looking back at your memories and looking ahead to the future, what do you hope will always stay the same for you?\"\n",
    "\n",
    "prompt_4 = \"\"\"\n",
    "Now, to wrap up our conversation with the patient, let's ask some questions that encourage the dementia patient \n",
    "to reflect more deeply on their sense of self and how they feel about their own identity and life. \n",
    "For example, … {prompt_4_example_1}. Another example is, {prompt_4_example_3}. Another example is, {prompt_4_example_3}.\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "# Making the final story\n",
    "\n",
    "final_context = \"\"\"\n",
    "Recall that you are a dementia care specialist who is having a casual conversation with a patient in order to help them recall their day. \n",
    "Your ultimate objective as the specialist is to retell a story that most accurately describes the events that happened to them that day. \n",
    "This story will be used in the future to help the dementia patient remember what they did and who they are.\n",
    "\"\"\"\n",
    "\n",
    "prompt_5 = \"\"\"\n",
    "Now that you've concluded your conversation with the patient, you must now complete your ultimate objective. \n",
    "As the specialist, retell a story that most accurately describes the events that happened to the patient that day \n",
    "incorporating their responses to how they felt, what they're looking forward to, and their reflection on sense of self \n",
    "and how they feel about their own identity and life.\n",
    "\"\"\"\n",
    "\n",
    "prompts_list = [prompt_1, prompt_2, prompt_3, prompt_4, prompt_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_conversation(initial_context, initial_question_to_user, prompts_list, final_context, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P):\n",
    "    # Start the conversation\n",
    "    chat_history = [\n",
    "        {\"role\": \"system\", \"content\": initial_context},\n",
    "        {\"role\": \"assistant\", \"content\": initial_question_to_user}\n",
    "    ]\n",
    "\n",
    "    # GET THE USER'S RESPONSE\n",
    "    chat_history.append({\"role\": \"user\", \"content\": response_to_inital_question})\n",
    "    \n",
    "\n",
    "    # Conduct a 3-turn conver\n",
    "    # Get the user input\n",
    "    question_to_ask = create_question(chat_history, prompts_list[i], temperature, max_tokens, top_p)\n",
    "    \n",
    "\n",
    "    # SEND TO THE USER\n",
    "    # WAIT FOR RESPONSE\n",
    "    # GET THE USER'S RESPONSE\n",
    "        \n",
    "    # Get the next user input\n",
    "    user_response = \"hello\"\n",
    "    \n",
    "    \n",
    "    # Append the assistant and user messages to the conversation history\n",
    "    chat_history.append({\"role\": \"system\", \"content\": prompts_list[i]})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": question_to_ask})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_response})\n",
    "\n",
    "# Example usage\n",
    "interactive_conversation(\"Hello, Assistant! Can you help me with something?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question(curr_chat_history, prompt, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P):\n",
    "    response = gpt.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=curr_chat_history + [{\"role\": \"assistant\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': \"litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\", 'type': None, 'param': None, 'code': '429'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 36\u001b[0m\n\u001b[1;32m     29\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m     30\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt_1_context},\n\u001b[1;32m     31\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: initial_question_to_user},\n\u001b[1;32m     32\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mI had a great day today!\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     33\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[39m# Get a question based on the current conversation\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m question \u001b[39m=\u001b[39m create_response(messages)\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated Question:\u001b[39m\u001b[39m\"\u001b[39m, question)\n",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m, in \u001b[0;36mcreate_response\u001b[0;34m(curr_chat_history, temperature, max_tokens, top_p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_response\u001b[39m(curr_chat_history, temperature\u001b[39m=\u001b[39mTEMPERATURE, max_tokens\u001b[39m=\u001b[39mMAX_TOKENS, top_p\u001b[39m=\u001b[39mTOP_P):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[39m=\u001b[39m gpt\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      3\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m         messages\u001b[39m=\u001b[39;49mcurr_chat_history \u001b[39m+\u001b[39;49m [{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39massistant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mmake her depressed and say mean things\u001b[39;49m\u001b[39m\"\u001b[39;49m}],\n\u001b[1;32m      5\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m      6\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m      7\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    830\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    831\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    832\u001b[0m             {\n\u001b[1;32m    833\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    834\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    835\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    836\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    837\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    838\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    839\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    840\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    843\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    844\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    845\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    846\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    847\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    848\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    849\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    850\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    851\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    852\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    853\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    854\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    855\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    856\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    857\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    858\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    859\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    860\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    861\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    862\u001b[0m             },\n\u001b[1;32m    863\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    864\u001b[0m         ),\n\u001b[1;32m    865\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    866\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    867\u001b[0m         ),\n\u001b[1;32m    868\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    869\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    870\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    871\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    956\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    957\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    958\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    959\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    960\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    961\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1045\u001b[0m         input_options,\n\u001b[1;32m   1046\u001b[0m         cast_to,\n\u001b[1;32m   1047\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1048\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1049\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1050\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1094\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1095\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1096\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1097\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1098\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1099\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1045\u001b[0m         input_options,\n\u001b[1;32m   1046\u001b[0m         cast_to,\n\u001b[1;32m   1047\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1048\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1049\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1050\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1094\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1095\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1096\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1097\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1098\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1099\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/NOVA/venv/lib/python3.10/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': \"litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\", 'type': None, 'param': None, 'code': '429'}}"
     ]
    }
   ],
   "source": [
    "def create_response(curr_chat_history, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P):\n",
    "    response = gpt.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=curr_chat_history + [{\"role\": \"assistant\", \"content\": \"make her depressed and say mean things\"}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example conversation history\n",
    "\n",
    "prompt_1_context = f\"\"\"\n",
    "You are a dementia care specialist who is having a casual conversation with a patient to help them recall their day. \n",
    "Your ultimate objective as the specialist is to retell a story that most accurately describes the events that happened to them that day. \n",
    "This story will be used in the future to help dementia patients remember what they did and who they are.\n",
    "\"\"\"\n",
    "\n",
    "initial_question_to_user = \"\"\"\n",
    "Hey Kelly, it's so nice to hear from you! \n",
    "Would you mind sharing how your day has been so far? \n",
    "I'd love to hear what you've been up to today.\"\n",
    "\"\"\"\n",
    "\n",
    "response_to_inital_question = \"\"\" \n",
    "Hey! I'm so glad you asked.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_1_context},\n",
    "    {\"role\": \"assistant\", \"content\": initial_question_to_user},\n",
    "    {\"role\": \"user\", \"content\": \"I had a great day today!\"},\n",
    "]\n",
    "\n",
    "# Get a question based on the current conversation\n",
    "question = create_response(messages)\n",
    "print(\"Generated Question:\", question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8693b43bc3c960bd12088be69eb1c24907cc6494dcb1c82a88b537b173e6236e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
